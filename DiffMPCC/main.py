from DiffMPCC.MPCCsolver import (
    MPCConfigDYN,
    STMPCCPlannerCasadi,
    lookup_phi,
    lookup_xy,
    n_neighbors,
)

import json
import pandas as pd
import numpy as onp
import jax
import jax.numpy as jnp


def rollout_dynamics(planner_dyn_mpc, init_state, model_param, q, T):
    """
    Roll out closed-loop dynamics for T steps.

    At each step, control is generated by planner_dyn_mpc.plan(state, model_param, q),
    then propagated through planner_dyn_mpc.predictive_model with Euler integration.

    Returns:
        states: (T+1, 7)
        actions: (T, 2) where action[:, 0] is acceleration [m/s^2].
    """
    state = onp.asarray(init_state, dtype=float)
    states = [state.copy()]
    actions = []

    for _ in range(T):
        u = onp.asarray(planner_dyn_mpc.plan(state, model_param, q), dtype=float).reshape(-1)
        actions.append(u)

        # predictive_model expects [Fxr, delta_v]; plan returns accel normalized by MASS.
        control_input = onp.array([u[0] * planner_dyn_mpc.config.MASS, u[1]], dtype=float)

        xdot = planner_dyn_mpc.predictive_model(state, control_input, model_param)
        xdot = onp.asarray(xdot, dtype=float).reshape(-1)
        state = state + planner_dyn_mpc.config.DTK * xdot
        states.append(state.copy())

    return jnp.asarray(states), jnp.asarray(actions)


def outer_parameter_objective(planner_dyn_mpc, init_state, model_param, q, T):
    """
    Outer objective: trajectory cost accumulated over T rollout steps.

    Cost uses contouring and lag errors to the track reference, plus
    a small control effort regularization.
    """
    states, actions = rollout_dynamics(planner_dyn_mpc, init_state, model_param, q, T)

    total_cost = 0.0

    for t in range(T):
        x_t, y_t = float(states[t, 0]), float(states[t, 1])
        x_next, y_next = float(states[t + 1, 0]), float(states[t + 1, 1])
        theta_t = planner_dyn_mpc.look_theta.query(x_t, y_t, k_neighbors=n_neighbors)
        theta_next = planner_dyn_mpc.look_theta.query(x_next, y_next, k_neighbors=n_neighbors)
        x_ref, y_ref = lookup_xy(theta_t)
        phi_t = lookup_phi(theta_t)

        dx = x_t - x_ref
        dy = y_t - y_ref
        e_con = onp.sin(phi_t) * dx - onp.cos(phi_t) * dy
        e_lag = -onp.cos(phi_t) * dx - onp.sin(phi_t) * dy

        total_cost += q[0] * (e_con ** 2)
        total_cost += q[1] * (e_lag ** 2)
        total_cost += -q[2] * max(0.0, theta_next - theta_t)

        # Mild control effort penalty to avoid excessively aggressive actions.
        total_cost += 1e-3 * float(actions[t, 0] ** 2 + actions[t, 1] ** 2)

    return float(total_cost)


def objective_value_and_grad_mpc_param(
    planner_dyn_mpc,
    init_state,
    model_param,
    mpc_param,
    T,
):
    """
    Value and dJ/d(mpc_param) using jax.grad on a JAX objective
    computed from a fixed rollout.

    Returns:
        value: objective at current mpc_param
        grad: gradient with shape (3,)
    """
    states, actions = rollout_dynamics(planner_dyn_mpc, init_state, model_param, mpc_param, T)

    e_con2 = []
    e_lag2 = []
    dtheta = []
    for t in range(T):
        x_t, y_t = float(states[t, 0]), float(states[t, 1])
        x_next, y_next = float(states[t + 1, 0]), float(states[t + 1, 1])
        theta_t = planner_dyn_mpc.look_theta.query(x_t, y_t, k_neighbors=n_neighbors)
        theta_next = planner_dyn_mpc.look_theta.query(x_next, y_next, k_neighbors=n_neighbors)
        x_ref, y_ref = lookup_xy(theta_t)
        phi_t = lookup_phi(theta_t)

        dx = x_t - x_ref
        dy = y_t - y_ref
        e_con = onp.sin(phi_t) * dx - onp.cos(phi_t) * dy
        e_lag = -onp.cos(phi_t) * dx - onp.sin(phi_t) * dy

        e_con2.append(e_con ** 2)
        e_lag2.append(e_lag ** 2)
        dtheta.append(max(0.0, theta_next - theta_t))

    e_con2     = jnp.asarray(e_con2)
    e_lag2     = jnp.asarray(e_lag2)
    dtheta     = jnp.asarray(dtheta)
    action_reg = 1e-3 * jnp.sum(actions[:, 0] ** 2 + actions[:, 1] ** 2)

    def objective_wrt_mpc_param(q):
        return q[0] * jnp.sum(e_con2) + q[1] * jnp.sum(e_lag2) - q[2] * jnp.sum(dtheta) + action_reg

    mpc_param_jax = jnp.asarray(mpc_param)
    value = objective_wrt_mpc_param(mpc_param_jax)
    grad = jax.grad(objective_wrt_mpc_param)(mpc_param_jax)
    return float(value), grad


def policy_gradient_update_mpc_param(
    planner_dyn_mpc,
    init_state,
    model_param,
    mpc_param,
    T,
    lr=1e-3,
    iters=1,
):
    q = jnp.asarray(mpc_param)
    value = 0.0
    grad = jnp.zeros_like(q)
    for _ in range(iters):
        value, grad = objective_value_and_grad_mpc_param(
            planner_dyn_mpc=planner_dyn_mpc,
            init_state=init_state,
            model_param=model_param,
            mpc_param=q,
            T=T,
        )
        q = jnp.maximum(q - lr * grad, 1e-6)
    return q, value, grad


if __name__ == '__main__':
    map_file     = 'data/rounded_rectangle_waypoints.csv'
    tpamap_name  = 'data/rounded_rectangle_tpamap.csv'
    tpadata_name = 'data/rounded_rectangle_tpadata.json'
    tpamap = onp.loadtxt(tpamap_name, delimiter=';', skiprows=1)

    tpadata = {}
    with open(tpadata_name) as f:
        tpadata = json.load(f)

    raceline   = onp.loadtxt(map_file, delimiter=";", skiprows=3)
    waypoints  = jnp.asarray(raceline)


    with open('data/log_full_Vinit_8.0_c20.0_l3000.0_p100.0_weightslip0.5_thetaslip_100_150_290_310_non', 'r') as f:
        data = json.load(f)
    print(data.keys())
    dyn_config = MPCConfigDYN()
    dyn_config.q_contour = data['q_contour'][0]
    dyn_config.q_lag     = data['q_lag'][0]
    dyn_config.q_theta   = data['q_theta'][0]

    # Set up MPCC solver
    BR = data['BR'][0]
    CR = data['CR'][0]
    DR = data['DR'][0]*(9.81*dyn_config.MASS)/2
    BF = data['BF'][0]
    CF = data['CF'][0]
    DF = data['DF'][0]*(9.81*dyn_config.MASS)/2
    CM = data['CM'][0]
    model_param = jnp.array([BR, CR, DR, BF, CF, DF, CM])

    planner_dyn_mpc = STMPCCPlannerCasadi(waypoints=waypoints, config=dyn_config, param=model_param)

    # Test the planner with all states from the dataset
    X = jnp.array(data['x'])
    Y = jnp.array(data['y'])
    Yaw = jnp.array(data['yaw'])
    Yaw_rate = jnp.array(data['yaw_rate'])
    VX = jnp.array(data['vx'])
    VY = jnp.array(data['vy'])
    STR_angle = jnp.array(data['steer_angle'])


    horizon = 2 # Ensure we have enough data for the horizon
    T = 20
    pg_iters = 10
    for index in range(horizon):
        test_state = jnp.array([X[index], Y[index], VX[index], Yaw[index], VY[index], Yaw_rate[index], STR_angle[index]])

        formatted_state = [float(f"{value:.3f}") for value in onp.asarray(test_state)]
        print("Test state at index %d: %s" % (index, formatted_state,))

        mpc_param = jnp.array([data['q_contour'][index], data['q_lag'][index], data['q_theta'][index]])
        model_param = jnp.array([data['BR'][index], 
                                 data['CR'][index], 
                                 data['DR'][index]*(9.81*dyn_config.MASS)/2, 
                                 data['BF'][index], data['CF'][index], 
                                 data['DF'][index]*(9.81*dyn_config.MASS)/2, 
                                 data['CM'][index]])


        u = planner_dyn_mpc.plan(test_state, model_param, mpc_param)

        print("Optimal acceleration:",   u[0], "--Data acceleration:", data['acce'][index])
        print("Optimal steering speed:", u[1], "--Data steering speed:", data['steering_rate'][index+1])

        new_mpc_param, traj_cost, grad_mpc_param = policy_gradient_update_mpc_param(
            planner_dyn_mpc=planner_dyn_mpc,
            init_state=test_state,
            model_param=model_param,
            mpc_param=mpc_param,
            T=T,
            lr=1e-1,
            iters=pg_iters,
        )
        print(f"Outer trajectory cost over T={T}: {traj_cost:.6f}")
        print(f"grad mpc_param: {onp.asarray(grad_mpc_param)}")
        print(f"updated mpc_param after {pg_iters} iters: {onp.asarray(new_mpc_param)}\n")
